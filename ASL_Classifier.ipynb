{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASL_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "imjFlZrvRJbG",
        "_tVV7dl3MVk-",
        "IcRLDKDDvIsf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0ngle/wvu-engr-camp/blob/main/ASL_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oEXgjPnSDT6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/data'"
      ],
      "metadata": {
        "id": "qrAY9stiFGSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imjFlZrvRJbG"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_c2hZVQXSvu"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"asl\")\n",
        "os.makedirs(HOUSING_PATH, exist_ok=True)\n",
        "\n",
        "def get_csv_path(filename, path):\n",
        "  csv_path = os.path.join(path, filename)\n",
        "  return pd.read_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxAEnQvDH_X1"
      },
      "source": [
        "train_whole = get_csv_path('ASL_train.csv', PATH)\n",
        "test_whole = get_csv_path('ASL_test.csv', PATH)\n",
        "\n",
        "train_whole.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lvmiSu0HRMM"
      },
      "source": [
        "The image data is saved to the csv as strings of 1D arrays of length 2500. Because of how Pandas' to_csv works, there are new line characters every 8 entries. Below are the steps to convert these string back to numpy arrays\n",
        "\n",
        "1. Get rid of end brackets by getting the substring of each image from index 2 to index len(< str >) - 2\n",
        "\n",
        "2. Use numpy's .fromstring function on \\n separators to convert the string to an array\n",
        "\n",
        "3. Reshape the numpy array with .reshape to get the original 50x50 image\n",
        "\n",
        "After the list is filled, I am going to ditch the dataframe and convert all of the lists to numpy arrays. I had previously replaced the index in the dataframe with updated values, but that wasn't playing nice with model.fit() later. Sending them straight to numpy arrays with .asarray() should address this problem\n",
        "\n",
        "Another Note: I forgot that I need my labels to be integers and I'm not running the preprocessing step again, so I'm just going to encode them manually here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdsnvM9MCu4o"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "length = len(train_whole['images'])\n",
        "img_array_train = [None] * length\n",
        "label_array_train = [None] * length\n",
        "\n",
        "for i in range(0, length):\n",
        "  #Grab image\n",
        "  img = train_whole['images'][i]\n",
        "\n",
        "  #Get substring (omitting brackets)\n",
        "  img = img[2:len(img) - 2]\n",
        "\n",
        "  #Convert to array, reshape, and add to new list\n",
        "  img_array_train[i] = np.fromstring(img, sep='\\n').reshape(50, 50)\n",
        "\n",
        "  #Handling labels\n",
        "  label = train_whole['labels'][i]\n",
        "  if len(label) == 1:\n",
        "    label_array_train[i] = ord(label) - ord('A')\n",
        "  else:\n",
        "    if label == 'nothing':\n",
        "      label_array_train[i] = 26\n",
        "    elif label == 'space':\n",
        "      label_array_train[i] = 27\n",
        "    elif label == 'del':\n",
        "      label_array_train[i] = 28\n",
        "\n",
        "length = len(test_whole['images'])\n",
        "img_array_test = [None] * length\n",
        "label_array_test = [None] * length\n",
        "\n",
        "for i in range(0, length):\n",
        "  img = test_whole['images'][i]\n",
        "\n",
        "  img = img[2:len(img) - 2]\n",
        "\n",
        "  img_array_test[i] = np.fromstring(img, sep='\\n').reshape(50, 50)\n",
        "\n",
        "  label = test_whole['labels'][i]\n",
        "  if len(label) == 1:\n",
        "    label_array_test[i] = ord(label) - ord('A')\n",
        "  else:\n",
        "    if label == 'nothing':\n",
        "      label_array_test[i] = 26\n",
        "    elif label == 'space':\n",
        "      label_array_test[i] = 27\n",
        "    elif label == 'del':\n",
        "      label_array_test[i] = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iOTHaW9W7LQ"
      },
      "source": [
        "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "       'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
        "       'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space', 'del']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3iU_7m9oKct"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "#x represents data\n",
        "#y represents labels\n",
        "x_train_whole = shuffle(np.asarray(img_array_train), random_state=42)\n",
        "y_train_whole = shuffle(np.asarray(label_array_train), random_state=42)\n",
        "\n",
        "x_test, x_train = x_train_whole[:17000] / 255.0, x_train_whole[17000:] / 255.0\n",
        "y_test, y_train = y_train_whole[:17000], y_train_whole[17000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Iol4WisDAR"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqCuZ98pE-_2"
      },
      "source": [
        "Just confirming that the data is formatted correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6FV2F0FLSEB"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "\n",
        "def plot_sign(img, label):\n",
        "  plt.imshow(img, cmap='binary')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  print(\"Label: \", class_names[label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEpzT88KglhW"
      },
      "source": [
        "instance = 0\n",
        "plot_sign(x_train[instance], y_train[instance])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGEaHYeKG8E4"
      },
      "source": [
        "All of the data appears to be formatted and labeled correctly. The Kaggle contibutor incluced a test set, but there is only 1 image for each sign (totalling 29 images including del, nothing, and space).\n",
        "\n",
        "I'm going to make a new test set containing about 20% of all of the training data and then a validation set containing a fraction of what remains. This way there is adequate material to train, validate, and test on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tVV7dl3MVk-"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owh_RoLdqJGu"
      },
      "source": [
        "Here we are going to build the model. Before we do that, we need to establish what everything is and what it's doing.\n",
        "\n",
        "`models.keras.Sequential` - Defines the Sequential API. This just says we are going to pass our data straight through the network and that we aren't going to need the ability to send it our of order at all. It takes a `list` of layers as its input.\n",
        "\n",
        "`keras.layers.Flatten` - We can't feed a 2D matrix into a 1D vector, so we can use this layer to convert the inage into something the network can understand\n",
        "\n",
        "`keras.layers.Dense` - This is the most basic layer. It is a simply fully connected layer, meaning the output of every neuron in a layer is mapped to every neuron in the next layer\n",
        "\n",
        "ReLU and Softmax - Common activation functions used in most networks\n",
        "\n",
        "\n",
        "\n",
        "To define the model, we simply pass in a list of our layers. You can probably gather what all the inputs mean. For the most part the number of neurons we choose for each layer is aribtrary with the exception being the output layer. \n",
        "\n",
        "The output *must* have the same number of neurons as we have possible output classes. In the case of the ASL dataset we have 29 (26 letters + space + del + nothing)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "layers = [\n",
        "  Flatten(input_shape=[50, 50]),\n",
        "  Dense(300, activation=\"relu\"),  \n",
        "  Dense(100, activation=\"relu\"),\n",
        "  Dense(29, activation=\"softmax\") \n",
        "]\n",
        "\n",
        "model = Sequential(layers)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyUmzLUMYws"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epat1P51a-xh"
      },
      "source": [
        "We can use `.summary()` to show all the details of every layer.\n",
        "\n",
        "```\n",
        "model.summary()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKGehLYJMq0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHKhhW3kly6u"
      },
      "source": [
        "For reference, a model I am working on has 1,436,849,665 trainable parameters\n",
        "\n",
        "---\n",
        "\n",
        "`.compile()` is used to prep our network for training. Here we define our loss function, optimizer, and desired metrics.\n",
        "\n",
        "**sparse_categorical_crossentropy** - let's break this down. \"Crossentropy\" is a type of loss function which essentially figured out the distance our output is from the correct answer. \"categorical\" suggests we are working with numberical labels which corespond with some category list. By default, categorical crossentropy expects your inputs to be one-hot encoded. Ours are not. Thus we need to specific that our intputs are \"sparse\"\n",
        "\n",
        "**one-hot encoding** - If I remember correctly it works like this. Lets say we have n=4 possible categories (for simplicity). As it stands our labels are numbers that represent each of the 4 outputs. So we could get either a 1, 2, 3, or 4 as output. One-hot converts this into an array with length = n. Now it's a binary problem. So our four options are [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], or [0, 0, 0, 1]\n",
        "\n",
        "**sgd** - Short for \"Stochastic Gradient Descent\". There is some information on this in the Extra Content section of the powerpoint if you are interested :)\n",
        "\n",
        "---\n",
        "```\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lV1KVOjNRz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r2I2bGUcYtx"
      },
      "source": [
        "`.fit` just starts the training loop. We pass in our training set, x_train, and our training labels, y_train\n",
        "\n",
        "*epochs* - Essentially the amount of times we are going to loop through the whole training loop. So here we are going to run through all of the images 30 times.\n",
        "\n",
        "*validation_split* - During training, it is generally bad to test your results with your testing set. Once the model has seen the image in the test set it can end up having a bias toward those images because it's seen them before. Instead, we can use this to create what's called a \"validation set\". This is just a subset of our training set used for periodic testing to give us feedback on what's going on. It serves as a good estimate of how it'll perform on the testing set without spoiling the test set itself\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "history = model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry_9zOiZNebw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjb4FMXkpqzX"
      },
      "source": [
        "Notice the variable name we stored the results in, `history`. \n",
        "\n",
        "History is a neat thing with Keras that automatically saves the results from each epoch. So here we can create a datafrom from the history and plot it.\n",
        "\n",
        "We would expect the loss to decline overtime and the accuracy to increase.\n",
        "\n",
        "When analyzing graphs be sure to keep convergence in mind. If you don't know what convergence means it's basically just the graph gradually getting closer and closer to a some value. We don't want the graph to oscillate at all - that suggest something could be wrong with the network, training loop, loss function, optimizer, input preprocessing, etc, etc, etc. \n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv7YTNkqTbMv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRLDKDDvIsf"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIHuRFNIvaAy"
      },
      "source": [
        "To get some predictions, we can call `.predict`. Just pass in the testing set and it'll spit out some predictions - though these may not be exactly what you'd expect.\n",
        "\n",
        "Say we feed a single image of the letter B to the model. Instead of spitting out '1' (the index of B in our class list), instead it will give us a list of 29 elements. Each of these elements correspond to a certainty that that index is the correct one. So it'll look something like this:\n",
        "\n",
        "[0.04, .98, .23, .01, .06, .10, ...]\n",
        "\n",
        "So we need a way to grab the index with the highest certainty. Luckily `np.argmax` exists. This is a numpy function that just gives you the index of the largest element.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
        "y_pred\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzPqU_43vPkg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6rTjO2e6uJ"
      },
      "source": [
        "Now we have an array of index predictions and an array of labels. Now it's super easy to determine the accuracy. We can just compare each index of our prediction list to the same index in our label list and bang, we have an accuracy.\n",
        "\n",
        "```\n",
        "total = 0\n",
        "correct = 0\n",
        "for i in range(0, len(y_test)):\n",
        "  total += 1\n",
        "\n",
        "  if (y_pred[i] == y_test[i]):\n",
        "    correct += 1\n",
        "\n",
        "print(\"Accuracy: \", correct / total)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mSsaIuPz6BD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b9LSADQKxfj"
      },
      "source": [
        "And numbers are cool and all, but it's always cooler to actually see the results. This loop just prints out an image, its predicted label, and its actual label\n",
        "\n",
        "```\n",
        "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "       'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
        "       'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space', 'del']\n",
        "\n",
        "for i in range(0, 10):\n",
        "  plot_sign(x_test[i], y_test[i])\n",
        "  index = y_pred[i]\n",
        "  print(\"Pred :\", class_names[index])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Fc2KsJKs84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}